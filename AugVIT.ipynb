{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install einops"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w8EigkTR7sJ",
        "outputId": "6ea38cf0-4265-4628-bf42-dc31d6c5ffdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.6 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer\n",
        "from tensorflow.keras import Sequential\n",
        "import tensorflow.keras.layers as nn\n",
        "\n",
        "from tensorflow import einsum\n",
        "from einops import rearrange, repeat\n",
        "from einops.layers.tensorflow import Rearrange\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "uVXTk49tR4li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pair(t):\n",
        "    return t if isinstance(t, tuple) else (t, t)\n",
        "def gelu(x):\n",
        "   \n",
        "    cdf = 0.5 * (1.0 + tf.tanh(\n",
        "        (np.sqrt(2 / np.pi) * (x + 0.044715 * tf.pow(x, 3)))))\n",
        "    return x * cdf\n",
        "\n",
        "class PreNorm(Layer):\n",
        "    def __init__(self, fn):\n",
        "        super(PreNorm, self).__init__()\n",
        "        self.norm = nn.LayerNormalization()\n",
        "        self.fn = fn\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return self.fn(self.norm(x), training=training)\n",
        "\n",
        "\n",
        "class MLP(Layer):\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.0):\n",
        "        super(MLP, self).__init__()\n",
        "        self.net = Sequential([\n",
        "            nn.Dense(units=hidden_dim,activation=gelu),\n",
        "            \n",
        "            nn.Dropout(rate=dropout),\n",
        "            nn.Dense(units=dim),\n",
        "            nn.Dropout(rate=dropout)\n",
        "        ])\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        return self.net(x, training=training)\n",
        "\n",
        "class Attention(Layer):\n",
        "    def __init__(self, dim, heads=8, dim_head=64, dropout=0.0):\n",
        "        super(Attention, self).__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        project_out = not (heads == 1 and dim_head == dim)\n",
        "\n",
        "        self.heads = heads\n",
        "        self.scale = dim_head ** -0.5\n",
        "\n",
        "        self.attend = nn.Softmax()\n",
        "        self.to_qkv = nn.Dense(units=inner_dim * 3, use_bias=False)\n",
        "\n",
        "        if project_out:\n",
        "            self.to_out = [\n",
        "                nn.Dense(units=dim),\n",
        "                nn.Dropout(rate=dropout)\n",
        "            ]\n",
        "        else:\n",
        "            self.to_out = []\n",
        "\n",
        "        self.to_out = Sequential(self.to_out)\n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        qkv = self.to_qkv(x)\n",
        "        qkv = tf.split(qkv, num_or_size_splits=3, axis=-1)\n",
        "        q, k, v = map(lambda t: rearrange(t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
        "\n",
        "        # dots = tf.matmul(q, tf.transpose(k, perm=[0, 1, 3, 2])) * self.scale\n",
        "        dots = einsum('b h i d, b h j d -> b h i j', q, k) * self.scale\n",
        "        attn = self.attend(dots)\n",
        "\n",
        "        # x = tf.matmul(attn, v)\n",
        "        x = einsum('b h i j, b h j d -> b h i d', attn, v)\n",
        "        x = rearrange(x, 'b h n d -> b n (h d)')\n",
        "        x = self.to_out(x, training=training)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Transformer(Layer):\n",
        "    def __init__(self, dim, depth, heads, dim_head, mlp_dim, dropout=0.0):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.layers = []\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append([\n",
        "                PreNorm(Attention(dim, heads=heads, dim_head=dim_head, dropout=dropout)),\n",
        "                PreNorm(nn.Dense(dim,activation=gelu)),\n",
        "                PreNorm(MLP(dim, mlp_dim, dropout=dropout)),\n",
        "                PreNorm(nn.Dense(dim,activation=gelu)),\n",
        "            ])\n",
        "        \n",
        "\n",
        "    def call(self, x, training=True):\n",
        "        for attn,aug_attn, mlp, augs in self.layers:\n",
        "            x = attn(x, training=training) + x + aug_attn(x, training=training)\n",
        "            x = mlp(x, training=training) + x + augs(x, training=training)\n",
        "        return x\n",
        "\n",
        "class AUGViT(Model):\n",
        "    def __init__(self, image_size, patch_size, num_classes, dim, depth, heads, mlp_dim,\n",
        "                 pool='cls', dim_head=64, dropout=0.0, emb_dropout=0.0):\n",
        "\n",
        "        super(AUGViT, self).__init__()\n",
        "\n",
        "        image_height, image_width = pair(image_size)\n",
        "        patch_height, patch_width = pair(patch_size)\n",
        "\n",
        "        assert image_height % patch_height == 0 and image_width % patch_width == 0, 'Image dimensions must be divisible by the patch size.'\n",
        "\n",
        "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
        "        assert pool in {'cls', 'mean'}, 'pool type must be either cls (cls token) or mean (mean pooling)'\n",
        "\n",
        "        self.patch_embedding = Sequential([\n",
        "            Rearrange('b (h p1) (w p2) c -> b (h w) (p1 p2 c)', p1=patch_height, p2=patch_width),\n",
        "            nn.Dense(units=dim)\n",
        "        ], name='patch_embedding')\n",
        "\n",
        "        self.pos_embedding = tf.Variable(initial_value=tf.random.normal([1, num_patches + 1, dim]))\n",
        "        self.cls_token = tf.Variable(initial_value=tf.random.normal([1, 1, dim]))\n",
        "        self.dropout = nn.Dropout(rate=emb_dropout)\n",
        "\n",
        "        self.transformer = Transformer(dim, depth, heads, dim_head, mlp_dim, dropout)\n",
        "\n",
        "        self.pool = pool\n",
        "\n",
        "        self.mlp_head = Sequential([\n",
        "            nn.LayerNormalization(),\n",
        "            nn.Dense(units=num_classes)\n",
        "        ], name='mlp_head')\n",
        "\n",
        "    def call(self, img, training=True, **kwargs):\n",
        "        x = self.patch_embedding(img)\n",
        "        b, n, d = x.shape\n",
        "\n",
        "        cls_tokens = repeat(self.cls_token, '() n d -> b n d', b=b)\n",
        "        x = tf.concat([cls_tokens, x], axis=1)\n",
        "        x += self.pos_embedding[:, :(n + 1)]\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        x = self.transformer(x, training=training)\n",
        "\n",
        "        if self.pool == 'mean':\n",
        "            x = tf.reduce_mean(x, axis=1)\n",
        "        else:\n",
        "            x = x[:, 0]\n",
        "\n",
        "        x = self.mlp_head(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "c8JCW3Aoob6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AUGViT(\n",
        "    image_size = 32,\n",
        "    patch_size = 4,\n",
        "    num_classes = 10,\n",
        "    dim = 128,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 256,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n"
      ],
      "metadata": {
        "id": "Uvn2mN_G9PAM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "num_classes = 10\n",
        "input_shape = (32, 32, 3)\n",
        "batch_size = 32\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RYBkNJqCJor_",
        "outputId": "45822ca0-ebd7-4612-b24b-60edfa0f314c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (50000, 32, 32, 3) - y_train shape: (50000, 10)\n",
            "x_test shape: (10000, 32, 32, 3) - y_test shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "XuEZkSAIMbbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.Adam()\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
        "val_acc_metric = keras.metrics.CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "7HPPJvGPMjmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n",
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n",
        "import time\n",
        "\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "    train_acc_metric.reset_states()\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_states()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TycS5BAkXmGq",
        "outputId": "b5e78b5c-e394-46fc-8df1-bc3c95c4f8ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 3.0022\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.8922\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 1.8452\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.6563\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 1.4895\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.4970\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.2341\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.5623\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.3758\n",
            "Validation acc: 0.4396\n",
            "Time taken: 90.02s\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 1.6908\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.1552\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 1.7971\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.5269\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 1.3688\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.2760\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.1611\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.2627\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.4906\n",
            "Validation acc: 0.4855\n",
            "Time taken: 78.10s\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 1.2350\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.4549\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 1.1369\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.4098\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 1.3325\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.3326\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.5432\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.2889\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.5214\n",
            "Validation acc: 0.5123\n",
            "Time taken: 78.29s\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 1.2304\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 0.8663\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 1.4065\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.1509\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 1.1316\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.1801\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.5149\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.3027\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.5433\n",
            "Validation acc: 0.5292\n",
            "Time taken: 78.43s\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 1.0988\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.2603\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 1.1236\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.0008\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 0.9520\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.2585\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.0837\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.4636\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.5608\n",
            "Validation acc: 0.5590\n",
            "Time taken: 78.54s\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 1.1477\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.3913\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 0.9050\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.4354\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 0.9444\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.3286\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.0991\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.4193\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.5793\n",
            "Validation acc: 0.5774\n",
            "Time taken: 78.58s\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 1.3564\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.1388\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 1.3250\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 0.9328\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 0.9342\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.2192\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.3356\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.4435\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.5921\n",
            "Validation acc: 0.5656\n",
            "Time taken: 78.63s\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 1.1744\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.3396\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 1.1081\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.0563\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 1.0650\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 0.7804\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.2136\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.0063\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.6066\n",
            "Validation acc: 0.5932\n",
            "Time taken: 78.56s\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 1.1850\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 0.9836\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 0.9184\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.1290\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 0.9829\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.3042\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 0.6018\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 0.8390\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.6188\n",
            "Validation acc: 0.5765\n",
            "Time taken: 78.36s\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 0.7377\n",
            "Seen so far: 32 samples\n",
            "Training loss (for one batch) at step 200: 1.0696\n",
            "Seen so far: 6432 samples\n",
            "Training loss (for one batch) at step 400: 0.8858\n",
            "Seen so far: 12832 samples\n",
            "Training loss (for one batch) at step 600: 1.0603\n",
            "Seen so far: 19232 samples\n",
            "Training loss (for one batch) at step 800: 1.1856\n",
            "Seen so far: 25632 samples\n",
            "Training loss (for one batch) at step 1000: 1.0844\n",
            "Seen so far: 32032 samples\n",
            "Training loss (for one batch) at step 1200: 1.0571\n",
            "Seen so far: 38432 samples\n",
            "Training loss (for one batch) at step 1400: 1.4041\n",
            "Seen so far: 44832 samples\n",
            "Training acc over epoch: 0.6329\n",
            "Validation acc: 0.5945\n",
            "Time taken: 78.23s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Evx-fSliBx_v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}